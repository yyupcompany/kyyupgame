/**
 * AIä¼˜åŒ–æŸ¥è¯¢æœåŠ¡
 * é›†æˆæ™ºèƒ½æ¨¡å‹è·¯ç”±å’Œå¹¶è¡Œå¤„ç†ä¼˜åŒ–
 */

import SmartModelRouterService, { QueryType } from './ai-smart-model-router.service';
import { unifiedAIBridge } from './unified-ai-bridge.service';
import AIQueryCacheService from './ai-query-cache.service';
import AIProgressEventService from './ai-progress-event.service';

export interface OptimizedQueryResult {
  type: string;
  data?: any;
  response?: string;
  metadata: {
    executionTime: number;
    usedModel: string;
    queryType: QueryType;
    complexity: number;
    estimatedTokens: number;
    actualTokens?: number;
    cacheHit: boolean;
    optimizationApplied: string[];
  };
  visualization?: any;
  sessionId?: string;
  queryLogId?: number;
}

export class AIOptimizedQueryService {
  private static instance: AIOptimizedQueryService;
  private cacheService: typeof AIQueryCacheService;
  private modelRouter: typeof SmartModelRouterService;
  private progressService: typeof AIProgressEventService;

  private constructor() {
    this.cacheService = AIQueryCacheService;
    this.modelRouter = SmartModelRouterService;
    this.progressService = AIProgressEventService;
  }

  public static getInstance(): AIOptimizedQueryService {
    if (!AIOptimizedQueryService.instance) {
      AIOptimizedQueryService.instance = new AIOptimizedQueryService();
    }
    return AIOptimizedQueryService.instance;
  }

  /**
   * ä¼˜åŒ–ç‰ˆæŸ¥è¯¢å¤„ç† - ä¸»è¦å…¥å£ç‚¹ (å¸¦å®æ—¶è¿›åº¦)
   */
  public async processOptimizedQuery(
    queryText: string,
    userId: number,
    sessionId?: string
  ): Promise<OptimizedQueryResult> {
    const startTime = Date.now();
    const optimizationApplied: string[] = [];
    const effectiveSessionId = sessionId || `query_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

    try {
      console.log('ğŸš€ [OptimizedAI] å¼€å§‹å¤„ç†ä¼˜åŒ–æŸ¥è¯¢:', queryText);

      // ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¼“å­˜
      await this.progressService.sendProgress(effectiveSessionId, 'cache_check', 'æ£€æŸ¥ç¼“å­˜ç»“æœ...', 35);
      const cachedResult = await this.cacheService.getCachedResult(queryText, userId);
      if (cachedResult) {
        await this.progressService.sendProgress(effectiveSessionId, 'cache_hit', 'æ‰¾åˆ°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›', 100);
        optimizationApplied.push('cache_hit');
        return {
          ...cachedResult,
          metadata: {
            ...cachedResult.metadata,
            optimizationApplied
          }
        };
      }

      // ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½æ¨¡å‹é€‰æ‹©
      await this.progressService.sendProgress(effectiveSessionId, 'model_select', 'åˆ†ææŸ¥è¯¢æ„å›¾å¹¶é€‰æ‹©æœ€ä¼˜AIæ¨¡å‹...', 25);
      const modelSelection = await this.modelRouter.selectOptimalModel(queryText);
      console.log('ğŸ¯ [OptimizedAI] é€‰æ‹©æ¨¡å‹:', modelSelection.modelName);

      // ç¬¬ä¸‰æ­¥ï¼šå¹¶è¡Œå¤„ç†ä¼˜åŒ– (å¸¦å®æ—¶è¿›åº¦)
      const result = await this.executeOptimizedQueryWithProgress(
        queryText,
        userId,
        effectiveSessionId,
        modelSelection,
        optimizationApplied
      );

      const executionTime = Date.now() - startTime;
      console.log(`âš¡ [OptimizedAI] æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: ${executionTime}ms`);

      // ä¿å­˜åˆ°ç¼“å­˜
      await this.cacheService.saveQueryResult(
        queryText,
        userId,
        result.type === 'data_query' ? 'data_query' : 'ai_response',
        result,
        effectiveSessionId,
        modelSelection.modelName,
        executionTime
      );

      // å®Œæˆè¿›åº¦
      await this.progressService.sendProgress(effectiveSessionId, 'complete', 'æŸ¥è¯¢å®Œæˆ', 100);

      return {
        ...result,
        metadata: {
          ...result.metadata,
          executionTime,
          optimizationApplied
        }
      };

    } catch (error) {
      console.error('âŒ [OptimizedAI] æŸ¥è¯¢å¤„ç†å¤±è´¥:', error);
      if (this.progressService.getActiveSession(effectiveSessionId)) {
        this.progressService.handleProgressError(effectiveSessionId, error as Error);
      }
      throw error;
    }
  }

  /**
   * æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢ (å¸¦å®æ—¶è¿›åº¦åé¦ˆ)
   */
  private async executeOptimizedQueryWithProgress(
    queryText: string,
    userId: number,
    sessionId: string,
    modelSelection: any,
    optimizationApplied: string[]
  ): Promise<OptimizedQueryResult> {
    const { analysis } = modelSelection;

    // ç¡®å®šæŸ¥è¯¢å¤æ‚åº¦ç±»å‹
    const complexityType = analysis.complexity <= 3 ? 'simple' :
                          analysis.complexity <= 6 ? 'medium' : 'complex';

    // è·å–å¯¹åº”çš„è¿›åº¦æ­¥éª¤
    const steps = this.progressService.getQuerySteps(complexityType);

    // åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    this.progressService.startProgressTracking({
      sessionId,
      queryId: `query_${Date.now()}`,
      userId,
      totalSteps: steps.length,
      onComplete: (result) => {
        console.log(`âœ… [Progress] æŸ¥è¯¢ ${sessionId} å®Œæˆ`);
      },
      onError: (error) => {
        console.error(`âŒ [Progress] æŸ¥è¯¢ ${sessionId} å¤±è´¥:`, error);
      }
    });

    // æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥å¹¶å‘é€è¿›åº¦
    switch (analysis.type) {
      case QueryType.COUNT:
      case QueryType.STATUS_CHECK:
        optimizationApplied.push('ultra_fast_model');
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œå¿«é€ŸæŸ¥è¯¢...', 70);
        return await this.handleSimpleQuery(queryText, modelSelection);

      case QueryType.SIMPLE_QUESTION:
        optimizationApplied.push('fast_response_model');
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡ŒAIé—®ç­”...', 70);
        return await this.handleSimpleQuestion(queryText, modelSelection);

      case QueryType.BASIC_EXPLANATION:
        optimizationApplied.push('medium_fast_model');
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œè§£é‡ŠæŸ¥è¯¢...', 70);
        return await this.handleBasicExplanation(queryText, modelSelection);

      case QueryType.DATA_QUERY:
        optimizationApplied.push('standard_model');
        await this.progressService.sendProgress(sessionId, 'data_prepare', 'å‡†å¤‡æŸ¥è¯¢æ•°æ®...', 45);
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œæ•°æ®æŸ¥è¯¢...', 75);
        return await this.handleDataQuery(queryText, userId, sessionId, modelSelection);

      case QueryType.ANALYSIS:
        optimizationApplied.push('thinking_model');
        await this.progressService.sendProgress(sessionId, 'data_prepare', 'å‡†å¤‡åˆ†ææ•°æ®...', 45);
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œæ·±åº¦åˆ†æ...', 75);
        return await this.handleAnalysis(queryText, userId, sessionId, modelSelection);

      case QueryType.TOOL_CALLING:
        optimizationApplied.push('tool_model');
        await this.progressService.sendProgress(sessionId, 'data_prepare', 'å‡†å¤‡å·¥å…·è°ƒç”¨...', 45);
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œå·¥å…·è°ƒç”¨...', 75);
        return await this.handleToolCalling(queryText, userId, sessionId, modelSelection);

      default:
        optimizationApplied.push('default_model');
        await this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œé»˜è®¤æŸ¥è¯¢...', 70);
        return await this.handleDefaultQuery(queryText, userId, sessionId, modelSelection);
    }
  }

  /**
   * æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢
   */
  private async executeOptimizedQuery(
    queryText: string,
    userId: number,
    sessionId: string | undefined,
    modelSelection: any,
    optimizationApplied: string[]
  ): Promise<OptimizedQueryResult> {
    const { analysis } = modelSelection;

    // æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
    switch (analysis.type) {
      case QueryType.COUNT:
      case QueryType.STATUS_CHECK:
        optimizationApplied.push('ultra_fast_model');
        return await this.handleSimpleQuery(queryText, modelSelection);

      case QueryType.SIMPLE_QUESTION:
        optimizationApplied.push('fast_response_model');
        return await this.handleSimpleQuestion(queryText, modelSelection);

      case QueryType.BASIC_EXPLANATION:
        optimizationApplied.push('medium_fast_model');
        return await this.handleBasicExplanation(queryText, modelSelection);

      case QueryType.DATA_QUERY:
        optimizationApplied.push('standard_model');
        return await this.handleDataQuery(queryText, userId, sessionId, modelSelection);

      case QueryType.ANALYSIS:
        optimizationApplied.push('thinking_model');
        return await this.handleAnalysis(queryText, userId, sessionId, modelSelection);

      case QueryType.TOOL_CALLING:
        optimizationApplied.push('tool_model');
        return await this.handleToolCalling(queryText, userId, sessionId, modelSelection);

      default:
        optimizationApplied.push('default_model');
        return await this.handleDefaultQuery(queryText, userId, sessionId, modelSelection);
    }
  }

  /**
   * å¤„ç†ç®€å•æŸ¥è¯¢ï¼ˆç»Ÿè®¡ã€çŠ¶æ€æ£€æŸ¥ï¼‰
   */
  private async handleSimpleQuery(
    queryText: string,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    const { modelName, analysis } = modelSelection;

    // æ„å»ºä¼˜åŒ–æç¤ºè¯
    const optimizedPrompt = this.buildOptimizedPrompt(queryText, analysis.type);

    try {
      const response = await unifiedAIBridge.chat({
        model: modelName,
        messages: [
          {
            role: 'system',
            content: 'ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æŸ¥è¯¢åŠ©æ‰‹ã€‚è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”é—®é¢˜ï¼Œè¾“å‡ºé™åˆ¶åœ¨50å­—ä»¥å†…ã€‚'
          },
          {
            role: 'user',
            content: optimizedPrompt
          }
        ],
        temperature: 0.1,
        max_tokens: analysis.estimatedTokens
      });

      return {
        type: 'ai_response',
        response: response.data?.content || response.data?.message || 'æ— æ³•å¤„ç†æŸ¥è¯¢',
        metadata: {
          executionTime: 0,
          usedModel: modelName,
          queryType: analysis.type,
          complexity: analysis.complexity,
          estimatedTokens: analysis.estimatedTokens,
          actualTokens: response.data?.usage?.totalTokens || 0,
          cacheHit: false,
          optimizationApplied: []
        }
      };

    } catch (error) {
      console.error('âŒ [SimpleQuery] å¤„ç†å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * å¤„ç†ç®€å•é—®ç­”
   */
  private async handleSimpleQuestion(
    queryText: string,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    const { modelName, analysis } = modelSelection;

    const response = await unifiedAIBridge.chat({
      model: modelName,
      messages: [
        {
          role: 'system',
          content: 'è¯·ç›´æ¥å‡†ç¡®åœ°å›ç­”é—®é¢˜ï¼Œè¾“å‡ºé™åˆ¶åœ¨100å­—ä»¥å†…ã€‚'
        },
        {
          role: 'user',
          content: queryText
        }
      ],
      temperature: 0.2,
      max_tokens: analysis.estimatedTokens
    }, {
      endpointUrl: modelSelection.modelConfig.endpointUrl,
      apiKey: modelSelection.modelConfig.apiKey
    });

    return {
      type: 'ai_response',
      response: response.data?.content || response.data?.message || 'æ— æ³•å›ç­”é—®é¢˜',
      metadata: {
        executionTime: 0,
        usedModel: modelName,
        queryType: analysis.type,
        complexity: analysis.complexity,
        estimatedTokens: analysis.estimatedTokens,
        actualTokens: response.data?.usage?.totalTokens || 0,
        cacheHit: false,
        optimizationApplied: []
      }
    };
  }

  /**
   * å¤„ç†åŸºç¡€è§£é‡Š
   */
  private async handleBasicExplanation(
    queryText: string,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    const { modelName, analysis } = modelSelection;

    const response = await unifiedAIBridge.chat({
      model: modelName,
      messages: [
        {
          role: 'system',
          content: 'è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šé—®é¢˜ï¼Œè¾“å‡ºé™åˆ¶åœ¨200å­—ä»¥å†…ã€‚'
        },
        {
          role: 'user',
          content: queryText
        }
      ],
      temperature: 0.3,
      max_tokens: analysis.estimatedTokens
    }, {
      endpointUrl: modelSelection.modelConfig.endpointUrl,
      apiKey: modelSelection.modelConfig.apiKey
    });

    return {
      type: 'ai_response',
      response: response.data?.content || response.data?.message || 'æ— æ³•è§£é‡Š',
      metadata: {
        executionTime: 0,
        usedModel: modelName,
        queryType: analysis.type,
        complexity: analysis.complexity,
        estimatedTokens: analysis.estimatedTokens,
        actualTokens: response.data?.usage?.totalTokens || 0,
        cacheHit: false,
        optimizationApplied: []
      }
    };
  }

  /**
   * å¤„ç†æ•°æ®æŸ¥è¯¢
   */
  private async handleDataQuery(
    queryText: string,
    userId: number,
    sessionId: string | undefined,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    // è¿™é‡Œå¯ä»¥è°ƒç”¨åŸæœ‰çš„æ•°æ®æŸ¥è¯¢é€»è¾‘
    // ç›®å‰è¿”å›æ¨¡æ‹Ÿç»“æœ
    return {
      type: 'ai_response',
      response: `æ•°æ®æŸ¥è¯¢ç»“æœ: ${queryText}`,
      metadata: {
        executionTime: 0,
        usedModel: modelSelection.modelName,
        queryType: modelSelection.analysis.type,
        complexity: modelSelection.analysis.complexity,
        estimatedTokens: modelSelection.analysis.estimatedTokens,
        cacheHit: false,
        optimizationApplied: []
      }
    };
  }

  /**
   * å¤„ç†åˆ†ææŸ¥è¯¢
   */
  private async handleAnalysis(
    queryText: string,
    userId: number,
    sessionId: string | undefined,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    const { modelName, analysis } = modelSelection;

    const response = await unifiedAIBridge.chat({
      model: modelName,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·æä¾›è¯¦ç»†çš„åˆ†æç»“æœã€‚'
        },
        {
          role: 'user',
          content: queryText
        }
      ],
      temperature: 0.7,
      max_tokens: analysis.estimatedTokens
    }, {
      endpointUrl: modelSelection.modelConfig.endpointUrl,
      apiKey: modelSelection.modelConfig.apiKey
    });

    return {
      type: 'ai_response',
      response: response.data?.content || response.data?.message || 'æ— æ³•åˆ†æ',
      metadata: {
        executionTime: 0,
        usedModel: modelName,
        queryType: analysis.type,
        complexity: analysis.complexity,
        estimatedTokens: analysis.estimatedTokens,
        actualTokens: response.data?.usage?.totalTokens || 0,
        cacheHit: false,
        optimizationApplied: []
      }
    };
  }

  /**
   * å¤„ç†å·¥å…·è°ƒç”¨
   */
  private async handleToolCalling(
    queryText: string,
    userId: number,
    sessionId: string | undefined,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    // è¿™é‡Œå¯ä»¥å®ç°å·¥å…·è°ƒç”¨é€»è¾‘
    return {
      type: 'ai_response',
      response: `å·¥å…·è°ƒç”¨ç»“æœ: ${queryText}`,
      metadata: {
        executionTime: 0,
        usedModel: modelSelection.modelName,
        queryType: modelSelection.analysis.type,
        complexity: modelSelection.analysis.complexity,
        estimatedTokens: modelSelection.analysis.estimatedTokens,
        cacheHit: false,
        optimizationApplied: []
      }
    };
  }

  /**
   * å¤„ç†é»˜è®¤æŸ¥è¯¢
   */
  private async handleDefaultQuery(
    queryText: string,
    userId: number,
    sessionId: string | undefined,
    modelSelection: any
  ): Promise<OptimizedQueryResult> {
    const response = await unifiedAIBridge.chat({
      model: modelSelection.modelName,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚'
        },
        {
          role: 'user',
          content: queryText
        }
      ],
      temperature: 0.7,
      max_tokens: modelSelection.analysis.estimatedTokens
    }, {
      endpointUrl: modelSelection.modelConfig.endpointUrl,
      apiKey: modelSelection.modelConfig.apiKey
    });

    return {
      type: 'ai_response',
      response: response.data?.content || response.data?.message || 'æ— æ³•å¤„ç†',
      metadata: {
        executionTime: 0,
        usedModel: modelSelection.modelName,
        queryType: modelSelection.analysis.type,
        complexity: modelSelection.analysis.complexity,
        estimatedTokens: modelSelection.analysis.estimatedTokens,
        actualTokens: response.data?.usage?.totalTokens || 0,
        cacheHit: false,
        optimizationApplied: []
      }
    };
  }

  /**
   * æ„å»ºä¼˜åŒ–æç¤ºè¯
   */
  private buildOptimizedPrompt(queryText: string, queryType: QueryType): string {
    switch (queryType) {
      case QueryType.COUNT:
        return `è¯·ç»Ÿè®¡æŸ¥è¯¢: ${queryText}ã€‚åªéœ€è¿”å›æ•°å­—æˆ–ç®€è¦ç»“æœã€‚`;
      case QueryType.STATUS_CHECK:
        return `è¯·æ£€æŸ¥çŠ¶æ€: ${queryText}ã€‚åªéœ€è¿”å›"æ˜¯"æˆ–"å¦"ï¼Œæˆ–ç®€çŸ­çŠ¶æ€ã€‚`;
      default:
        return queryText;
    }
  }

  /**
   * è·å–æ€§èƒ½ç»Ÿè®¡
   */
  public getPerformanceStats(): any {
    return {
      modelRouter: this.modelRouter.getModelPerformanceStats(),
      cacheStats: this.cacheService.getCacheStats()
    };
  }
}

export default AIOptimizedQueryService.getInstance();