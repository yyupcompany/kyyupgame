/**
 * å¤šæ¨¡æ€æœåŠ¡
 * æ”¯æŒå›¾åƒã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€AIå¤„ç†
 */

import { unifiedAIBridge } from '../unified-ai-bridge.service';

export interface ImageAnalysisRequest {
  imageUrl?: string;
  imageBase64?: string;
  prompt?: string;
}

export interface ImageAnalysisResponse {
  description: string;
  labels?: string[];
  objects?: any[];
}

export interface AudioTranscribeRequest {
  audioUrl?: string;
  audioBuffer?: Buffer;
  language?: string;
}

export interface AudioTranscribeResponse {
  text: string;
  language?: string;
  duration?: number;
}

class MultimodalService {
  /**
   * åˆ†æå›¾åƒ
   */
  async analyzeImage(request: ImageAnalysisRequest): Promise<ImageAnalysisResponse> {
    console.log('ğŸ–¼ï¸ [å¤šæ¨¡æ€æœåŠ¡] åˆ†æå›¾åƒ');

    try {
      const response = await unifiedAIBridge.chat({
        model: 'doubao-vision',
        messages: [
          {
            role: 'user',
            content: request.prompt || 'è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹'
          }
        ],
        temperature: 0.7,
        max_tokens: 1000
      });

      const content = response.data?.content || response.data?.message || '';

      return {
        description: content,
        labels: [],
        objects: []
      };
    } catch (error: any) {
      console.error('âŒ [å¤šæ¨¡æ€æœåŠ¡] å›¾åƒåˆ†æå¤±è´¥:', error.message);
      throw error;
    }
  }

  /**
   * éŸ³é¢‘è½¬æ–‡å­—
   */
  async transcribeAudio(request: AudioTranscribeRequest): Promise<AudioTranscribeResponse> {
    console.log('ğŸ¤ [å¤šæ¨¡æ€æœåŠ¡] éŸ³é¢‘è½¬æ–‡å­—');

    try {
      const result = await unifiedAIBridge.processAudio({
        action: 'transcribe',
        file: request.audioBuffer,
        model: 'whisper-1'
      });

      return {
        text: result.data?.text || result.text || '',
        language: result.data?.language || result.language,
        duration: result.data?.duration || result.duration
      };
    } catch (error: any) {
      console.error('âŒ [å¤šæ¨¡æ€æœåŠ¡] éŸ³é¢‘è½¬æ–‡å­—å¤±è´¥:', error.message);
      throw error;
    }
  }

  /**
   * æ–‡å­—è½¬è¯­éŸ³
   */
  async textToSpeech(text: string, voice?: string): Promise<Buffer> {
    console.log('ğŸ”Š [å¤šæ¨¡æ€æœåŠ¡] æ–‡å­—è½¬è¯­éŸ³');

    try {
      const result = await unifiedAIBridge.processAudio({
        action: 'synthesize',
        file: text,
        model: voice || 'alloy'
      });

      if (result.success && result.audioBuffer) {
        return result.audioBuffer;
      }

      throw new Error(result.error || 'è¯­éŸ³åˆæˆå¤±è´¥');
    } catch (error: any) {
      console.error('âŒ [å¤šæ¨¡æ€æœåŠ¡] æ–‡å­—è½¬è¯­éŸ³å¤±è´¥:', error.message);
      throw error;
    }
  }

  /**
   * ç”Ÿæˆå›¾åƒ
   */
  async generateImage(options: any): Promise<any> {
    console.log('ğŸ¨ [å¤šæ¨¡æ€æœåŠ¡] ç”Ÿæˆå›¾åƒ');
    return {
      created: Date.now(),
      data: [{ url: '', b64_json: '' }]
    };
  }

  /**
   * è¯­éŸ³è½¬æ–‡å­—ï¼ˆOpenAIå…¼å®¹ï¼‰
   */
  async speechToText(options: any): Promise<any> {
    return this.transcribeAudio(options);
  }

  /**
   * è·å–æ”¯æŒçš„è¯­éŸ³åˆ—è¡¨
   */
  async getSupportedVoices(): Promise<string[]> {
    return ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'];
  }
}

export const multimodalService = new MultimodalService();
export default multimodalService;

