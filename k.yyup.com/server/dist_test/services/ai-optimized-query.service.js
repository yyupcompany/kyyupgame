"use strict";
/**
 * AIä¼˜åŒ–æŸ¥è¯¢æœåŠ¡
 * é›†æˆæ™ºèƒ½æ¨¡å‹è·¯ç”±å’Œå¹¶è¡Œå¤„ç†ä¼˜åŒ–
 */
var __assign = (this && this.__assign) || function () {
    __assign = Object.assign || function(t) {
        for (var s, i = 1, n = arguments.length; i < n; i++) {
            s = arguments[i];
            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))
                t[p] = s[p];
        }
        return t;
    };
    return __assign.apply(this, arguments);
};
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __generator = (this && this.__generator) || function (thisArg, body) {
    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;
    return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
    function verb(n) { return function (v) { return step([n, v]); }; }
    function step(op) {
        if (f) throw new TypeError("Generator is already executing.");
        while (g && (g = 0, op[0] && (_ = 0)), _) try {
            if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;
            if (y = 0, t) op = [op[0] & 2, t.value];
            switch (op[0]) {
                case 0: case 1: t = op; break;
                case 4: _.label++; return { value: op[1], done: false };
                case 5: _.label++; y = op[1]; op = [0]; continue;
                case 7: op = _.ops.pop(); _.trys.pop(); continue;
                default:
                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                    if (t[2]) _.ops.pop();
                    _.trys.pop(); continue;
            }
            op = body.call(thisArg, _);
        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
    }
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
exports.__esModule = true;
exports.AIOptimizedQueryService = void 0;
var ai_smart_model_router_service_1 = __importStar(require("./ai-smart-model-router.service"));
var ai_bridge_service_1 = require("./ai/bridge/ai-bridge.service");
var ai_query_cache_service_1 = __importDefault(require("./ai-query-cache.service"));
var ai_progress_event_service_1 = __importDefault(require("./ai-progress-event.service"));
var AIOptimizedQueryService = /** @class */ (function () {
    function AIOptimizedQueryService() {
        this.cacheService = ai_query_cache_service_1["default"];
        this.modelRouter = ai_smart_model_router_service_1["default"];
        this.progressService = ai_progress_event_service_1["default"];
    }
    AIOptimizedQueryService.getInstance = function () {
        if (!AIOptimizedQueryService.instance) {
            AIOptimizedQueryService.instance = new AIOptimizedQueryService();
        }
        return AIOptimizedQueryService.instance;
    };
    /**
     * ä¼˜åŒ–ç‰ˆæŸ¥è¯¢å¤„ç† - ä¸»è¦å…¥å£ç‚¹ (å¸¦å®æ—¶è¿›åº¦)
     */
    AIOptimizedQueryService.prototype.processOptimizedQuery = function (queryText, userId, sessionId) {
        return __awaiter(this, void 0, void 0, function () {
            var startTime, optimizationApplied, effectiveSessionId, cachedResult, modelSelection, result, executionTime, error_1;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        startTime = Date.now();
                        optimizationApplied = [];
                        effectiveSessionId = sessionId || "query_".concat(Date.now(), "_").concat(Math.random().toString(36).substr(2, 9));
                        _a.label = 1;
                    case 1:
                        _a.trys.push([1, 11, , 12]);
                        console.log('ğŸš€ [OptimizedAI] å¼€å§‹å¤„ç†ä¼˜åŒ–æŸ¥è¯¢:', queryText);
                        // ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¼“å­˜
                        return [4 /*yield*/, this.progressService.sendProgress(effectiveSessionId, 'cache_check', 'æ£€æŸ¥ç¼“å­˜ç»“æœ...', 35)];
                    case 2:
                        // ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¼“å­˜
                        _a.sent();
                        return [4 /*yield*/, this.cacheService.getCachedResult(queryText, userId)];
                    case 3:
                        cachedResult = _a.sent();
                        if (!cachedResult) return [3 /*break*/, 5];
                        return [4 /*yield*/, this.progressService.sendProgress(effectiveSessionId, 'cache_hit', 'æ‰¾åˆ°ç¼“å­˜ç»“æœï¼Œç›´æ¥è¿”å›', 100)];
                    case 4:
                        _a.sent();
                        optimizationApplied.push('cache_hit');
                        return [2 /*return*/, __assign(__assign({}, cachedResult), { metadata: __assign(__assign({}, cachedResult.metadata), { optimizationApplied: optimizationApplied }) })];
                    case 5: 
                    // ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½æ¨¡å‹é€‰æ‹©
                    return [4 /*yield*/, this.progressService.sendProgress(effectiveSessionId, 'model_select', 'åˆ†ææŸ¥è¯¢æ„å›¾å¹¶é€‰æ‹©æœ€ä¼˜AIæ¨¡å‹...', 25)];
                    case 6:
                        // ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½æ¨¡å‹é€‰æ‹©
                        _a.sent();
                        return [4 /*yield*/, this.modelRouter.selectOptimalModel(queryText)];
                    case 7:
                        modelSelection = _a.sent();
                        console.log('ğŸ¯ [OptimizedAI] é€‰æ‹©æ¨¡å‹:', modelSelection.modelName);
                        return [4 /*yield*/, this.executeOptimizedQueryWithProgress(queryText, userId, effectiveSessionId, modelSelection, optimizationApplied)];
                    case 8:
                        result = _a.sent();
                        executionTime = Date.now() - startTime;
                        console.log("\u26A1 [OptimizedAI] \u67E5\u8BE2\u5B8C\u6210\uFF0C\u8017\u65F6: ".concat(executionTime, "ms"));
                        // ä¿å­˜åˆ°ç¼“å­˜
                        return [4 /*yield*/, this.cacheService.saveQueryResult(queryText, userId, result.type === 'data_query' ? 'data_query' : 'ai_response', result, effectiveSessionId, modelSelection.modelName, executionTime)];
                    case 9:
                        // ä¿å­˜åˆ°ç¼“å­˜
                        _a.sent();
                        // å®Œæˆè¿›åº¦
                        return [4 /*yield*/, this.progressService.sendProgress(effectiveSessionId, 'complete', 'æŸ¥è¯¢å®Œæˆ', 100)];
                    case 10:
                        // å®Œæˆè¿›åº¦
                        _a.sent();
                        return [2 /*return*/, __assign(__assign({}, result), { metadata: __assign(__assign({}, result.metadata), { executionTime: executionTime, optimizationApplied: optimizationApplied }) })];
                    case 11:
                        error_1 = _a.sent();
                        console.error('âŒ [OptimizedAI] æŸ¥è¯¢å¤„ç†å¤±è´¥:', error_1);
                        if (this.progressService.getActiveSession(effectiveSessionId)) {
                            this.progressService.handleProgressError(effectiveSessionId, error_1);
                        }
                        throw error_1;
                    case 12: return [2 /*return*/];
                }
            });
        });
    };
    /**
     * æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢ (å¸¦å®æ—¶è¿›åº¦åé¦ˆ)
     */
    AIOptimizedQueryService.prototype.executeOptimizedQueryWithProgress = function (queryText, userId, sessionId, modelSelection, optimizationApplied) {
        return __awaiter(this, void 0, void 0, function () {
            var analysis, complexityType, steps, _a;
            return __generator(this, function (_b) {
                switch (_b.label) {
                    case 0:
                        analysis = modelSelection.analysis;
                        complexityType = analysis.complexity <= 3 ? 'simple' :
                            analysis.complexity <= 6 ? 'medium' : 'complex';
                        steps = this.progressService.getQuerySteps(complexityType);
                        // åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
                        this.progressService.startProgressTracking({
                            sessionId: sessionId,
                            queryId: "query_".concat(Date.now()),
                            userId: userId,
                            totalSteps: steps.length,
                            onComplete: function (result) {
                                console.log("\u2705 [Progress] \u67E5\u8BE2 ".concat(sessionId, " \u5B8C\u6210"));
                            },
                            onError: function (error) {
                                console.error("\u274C [Progress] \u67E5\u8BE2 ".concat(sessionId, " \u5931\u8D25:"), error);
                            }
                        });
                        _a = analysis.type;
                        switch (_a) {
                            case ai_smart_model_router_service_1.QueryType.COUNT: return [3 /*break*/, 1];
                            case ai_smart_model_router_service_1.QueryType.STATUS_CHECK: return [3 /*break*/, 1];
                            case ai_smart_model_router_service_1.QueryType.SIMPLE_QUESTION: return [3 /*break*/, 4];
                            case ai_smart_model_router_service_1.QueryType.BASIC_EXPLANATION: return [3 /*break*/, 7];
                            case ai_smart_model_router_service_1.QueryType.DATA_QUERY: return [3 /*break*/, 10];
                            case ai_smart_model_router_service_1.QueryType.ANALYSIS: return [3 /*break*/, 14];
                            case ai_smart_model_router_service_1.QueryType.TOOL_CALLING: return [3 /*break*/, 18];
                        }
                        return [3 /*break*/, 22];
                    case 1:
                        optimizationApplied.push('ultra_fast_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œå¿«é€ŸæŸ¥è¯¢...', 70)];
                    case 2:
                        _b.sent();
                        return [4 /*yield*/, this.handleSimpleQuery(queryText, modelSelection)];
                    case 3: return [2 /*return*/, _b.sent()];
                    case 4:
                        optimizationApplied.push('fast_response_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡ŒAIé—®ç­”...', 70)];
                    case 5:
                        _b.sent();
                        return [4 /*yield*/, this.handleSimpleQuestion(queryText, modelSelection)];
                    case 6: return [2 /*return*/, _b.sent()];
                    case 7:
                        optimizationApplied.push('medium_fast_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œè§£é‡ŠæŸ¥è¯¢...', 70)];
                    case 8:
                        _b.sent();
                        return [4 /*yield*/, this.handleBasicExplanation(queryText, modelSelection)];
                    case 9: return [2 /*return*/, _b.sent()];
                    case 10:
                        optimizationApplied.push('standard_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'data_prepare', 'å‡†å¤‡æŸ¥è¯¢æ•°æ®...', 45)];
                    case 11:
                        _b.sent();
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œæ•°æ®æŸ¥è¯¢...', 75)];
                    case 12:
                        _b.sent();
                        return [4 /*yield*/, this.handleDataQuery(queryText, userId, sessionId, modelSelection)];
                    case 13: return [2 /*return*/, _b.sent()];
                    case 14:
                        optimizationApplied.push('thinking_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'data_prepare', 'å‡†å¤‡åˆ†ææ•°æ®...', 45)];
                    case 15:
                        _b.sent();
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œæ·±åº¦åˆ†æ...', 75)];
                    case 16:
                        _b.sent();
                        return [4 /*yield*/, this.handleAnalysis(queryText, userId, sessionId, modelSelection)];
                    case 17: return [2 /*return*/, _b.sent()];
                    case 18:
                        optimizationApplied.push('tool_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'data_prepare', 'å‡†å¤‡å·¥å…·è°ƒç”¨...', 45)];
                    case 19:
                        _b.sent();
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œå·¥å…·è°ƒç”¨...', 75)];
                    case 20:
                        _b.sent();
                        return [4 /*yield*/, this.handleToolCalling(queryText, userId, sessionId, modelSelection)];
                    case 21: return [2 /*return*/, _b.sent()];
                    case 22:
                        optimizationApplied.push('default_model');
                        return [4 /*yield*/, this.progressService.sendProgress(sessionId, 'execute', 'æ‰§è¡Œé»˜è®¤æŸ¥è¯¢...', 70)];
                    case 23:
                        _b.sent();
                        return [4 /*yield*/, this.handleDefaultQuery(queryText, userId, sessionId, modelSelection)];
                    case 24: return [2 /*return*/, _b.sent()];
                }
            });
        });
    };
    /**
     * æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢
     */
    AIOptimizedQueryService.prototype.executeOptimizedQuery = function (queryText, userId, sessionId, modelSelection, optimizationApplied) {
        return __awaiter(this, void 0, void 0, function () {
            var analysis, _a;
            return __generator(this, function (_b) {
                switch (_b.label) {
                    case 0:
                        analysis = modelSelection.analysis;
                        _a = analysis.type;
                        switch (_a) {
                            case ai_smart_model_router_service_1.QueryType.COUNT: return [3 /*break*/, 1];
                            case ai_smart_model_router_service_1.QueryType.STATUS_CHECK: return [3 /*break*/, 1];
                            case ai_smart_model_router_service_1.QueryType.SIMPLE_QUESTION: return [3 /*break*/, 3];
                            case ai_smart_model_router_service_1.QueryType.BASIC_EXPLANATION: return [3 /*break*/, 5];
                            case ai_smart_model_router_service_1.QueryType.DATA_QUERY: return [3 /*break*/, 7];
                            case ai_smart_model_router_service_1.QueryType.ANALYSIS: return [3 /*break*/, 9];
                            case ai_smart_model_router_service_1.QueryType.TOOL_CALLING: return [3 /*break*/, 11];
                        }
                        return [3 /*break*/, 13];
                    case 1:
                        optimizationApplied.push('ultra_fast_model');
                        return [4 /*yield*/, this.handleSimpleQuery(queryText, modelSelection)];
                    case 2: return [2 /*return*/, _b.sent()];
                    case 3:
                        optimizationApplied.push('fast_response_model');
                        return [4 /*yield*/, this.handleSimpleQuestion(queryText, modelSelection)];
                    case 4: return [2 /*return*/, _b.sent()];
                    case 5:
                        optimizationApplied.push('medium_fast_model');
                        return [4 /*yield*/, this.handleBasicExplanation(queryText, modelSelection)];
                    case 6: return [2 /*return*/, _b.sent()];
                    case 7:
                        optimizationApplied.push('standard_model');
                        return [4 /*yield*/, this.handleDataQuery(queryText, userId, sessionId, modelSelection)];
                    case 8: return [2 /*return*/, _b.sent()];
                    case 9:
                        optimizationApplied.push('thinking_model');
                        return [4 /*yield*/, this.handleAnalysis(queryText, userId, sessionId, modelSelection)];
                    case 10: return [2 /*return*/, _b.sent()];
                    case 11:
                        optimizationApplied.push('tool_model');
                        return [4 /*yield*/, this.handleToolCalling(queryText, userId, sessionId, modelSelection)];
                    case 12: return [2 /*return*/, _b.sent()];
                    case 13:
                        optimizationApplied.push('default_model');
                        return [4 /*yield*/, this.handleDefaultQuery(queryText, userId, sessionId, modelSelection)];
                    case 14: return [2 /*return*/, _b.sent()];
                }
            });
        });
    };
    /**
     * å¤„ç†ç®€å•æŸ¥è¯¢ï¼ˆç»Ÿè®¡ã€çŠ¶æ€æ£€æŸ¥ï¼‰
     */
    AIOptimizedQueryService.prototype.handleSimpleQuery = function (queryText, modelSelection) {
        var _a, _b, _c;
        return __awaiter(this, void 0, void 0, function () {
            var modelName, analysis, optimizedPrompt, response, error_2;
            return __generator(this, function (_d) {
                switch (_d.label) {
                    case 0:
                        modelName = modelSelection.modelName, analysis = modelSelection.analysis;
                        optimizedPrompt = this.buildOptimizedPrompt(queryText, analysis.type);
                        _d.label = 1;
                    case 1:
                        _d.trys.push([1, 3, , 4]);
                        return [4 /*yield*/, ai_bridge_service_1.aiBridgeService.generateChatCompletion({
                                model: modelName,
                                messages: [
                                    {
                                        role: 'system',
                                        content: 'ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æŸ¥è¯¢åŠ©æ‰‹ã€‚è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”é—®é¢˜ï¼Œè¾“å‡ºé™åˆ¶åœ¨50å­—ä»¥å†…ã€‚'
                                    },
                                    {
                                        role: 'user',
                                        content: optimizedPrompt
                                    }
                                ],
                                temperature: 0.1,
                                max_tokens: analysis.estimatedTokens
                            }, {
                                endpointUrl: modelSelection.modelConfig.endpointUrl,
                                apiKey: modelSelection.modelConfig.apiKey
                            })];
                    case 2:
                        response = _d.sent();
                        return [2 /*return*/, {
                                type: 'ai_response',
                                response: ((_b = (_a = response.choices[0]) === null || _a === void 0 ? void 0 : _a.message) === null || _b === void 0 ? void 0 : _b.content) || 'æ— æ³•å¤„ç†æŸ¥è¯¢',
                                metadata: {
                                    executionTime: 0,
                                    usedModel: modelName,
                                    queryType: analysis.type,
                                    complexity: analysis.complexity,
                                    estimatedTokens: analysis.estimatedTokens,
                                    actualTokens: ((_c = response.usage) === null || _c === void 0 ? void 0 : _c.total_tokens) || 0,
                                    cacheHit: false,
                                    optimizationApplied: []
                                }
                            }];
                    case 3:
                        error_2 = _d.sent();
                        console.error('âŒ [SimpleQuery] å¤„ç†å¤±è´¥:', error_2);
                        throw error_2;
                    case 4: return [2 /*return*/];
                }
            });
        });
    };
    /**
     * å¤„ç†ç®€å•é—®ç­”
     */
    AIOptimizedQueryService.prototype.handleSimpleQuestion = function (queryText, modelSelection) {
        var _a, _b, _c;
        return __awaiter(this, void 0, void 0, function () {
            var modelName, analysis, response;
            return __generator(this, function (_d) {
                switch (_d.label) {
                    case 0:
                        modelName = modelSelection.modelName, analysis = modelSelection.analysis;
                        return [4 /*yield*/, ai_bridge_service_1.aiBridgeService.generateChatCompletion({
                                model: modelName,
                                messages: [
                                    {
                                        role: 'system',
                                        content: 'è¯·ç›´æ¥å‡†ç¡®åœ°å›ç­”é—®é¢˜ï¼Œè¾“å‡ºé™åˆ¶åœ¨100å­—ä»¥å†…ã€‚'
                                    },
                                    {
                                        role: 'user',
                                        content: queryText
                                    }
                                ],
                                temperature: 0.2,
                                max_tokens: analysis.estimatedTokens
                            }, {
                                endpointUrl: modelSelection.modelConfig.endpointUrl,
                                apiKey: modelSelection.modelConfig.apiKey
                            })];
                    case 1:
                        response = _d.sent();
                        return [2 /*return*/, {
                                type: 'ai_response',
                                response: ((_b = (_a = response.choices[0]) === null || _a === void 0 ? void 0 : _a.message) === null || _b === void 0 ? void 0 : _b.content) || 'æ— æ³•å›ç­”é—®é¢˜',
                                metadata: {
                                    executionTime: 0,
                                    usedModel: modelName,
                                    queryType: analysis.type,
                                    complexity: analysis.complexity,
                                    estimatedTokens: analysis.estimatedTokens,
                                    actualTokens: ((_c = response.usage) === null || _c === void 0 ? void 0 : _c.total_tokens) || 0,
                                    cacheHit: false,
                                    optimizationApplied: []
                                }
                            }];
                }
            });
        });
    };
    /**
     * å¤„ç†åŸºç¡€è§£é‡Š
     */
    AIOptimizedQueryService.prototype.handleBasicExplanation = function (queryText, modelSelection) {
        var _a, _b, _c;
        return __awaiter(this, void 0, void 0, function () {
            var modelName, analysis, response;
            return __generator(this, function (_d) {
                switch (_d.label) {
                    case 0:
                        modelName = modelSelection.modelName, analysis = modelSelection.analysis;
                        return [4 /*yield*/, ai_bridge_service_1.aiBridgeService.generateChatCompletion({
                                model: modelName,
                                messages: [
                                    {
                                        role: 'system',
                                        content: 'è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šé—®é¢˜ï¼Œè¾“å‡ºé™åˆ¶åœ¨200å­—ä»¥å†…ã€‚'
                                    },
                                    {
                                        role: 'user',
                                        content: queryText
                                    }
                                ],
                                temperature: 0.3,
                                max_tokens: analysis.estimatedTokens
                            }, {
                                endpointUrl: modelSelection.modelConfig.endpointUrl,
                                apiKey: modelSelection.modelConfig.apiKey
                            })];
                    case 1:
                        response = _d.sent();
                        return [2 /*return*/, {
                                type: 'ai_response',
                                response: ((_b = (_a = response.choices[0]) === null || _a === void 0 ? void 0 : _a.message) === null || _b === void 0 ? void 0 : _b.content) || 'æ— æ³•è§£é‡Š',
                                metadata: {
                                    executionTime: 0,
                                    usedModel: modelName,
                                    queryType: analysis.type,
                                    complexity: analysis.complexity,
                                    estimatedTokens: analysis.estimatedTokens,
                                    actualTokens: ((_c = response.usage) === null || _c === void 0 ? void 0 : _c.total_tokens) || 0,
                                    cacheHit: false,
                                    optimizationApplied: []
                                }
                            }];
                }
            });
        });
    };
    /**
     * å¤„ç†æ•°æ®æŸ¥è¯¢
     */
    AIOptimizedQueryService.prototype.handleDataQuery = function (queryText, userId, sessionId, modelSelection) {
        return __awaiter(this, void 0, void 0, function () {
            return __generator(this, function (_a) {
                // è¿™é‡Œå¯ä»¥è°ƒç”¨åŸæœ‰çš„æ•°æ®æŸ¥è¯¢é€»è¾‘
                // ç›®å‰è¿”å›æ¨¡æ‹Ÿç»“æœ
                return [2 /*return*/, {
                        type: 'ai_response',
                        response: "\u6570\u636E\u67E5\u8BE2\u7ED3\u679C: ".concat(queryText),
                        metadata: {
                            executionTime: 0,
                            usedModel: modelSelection.modelName,
                            queryType: modelSelection.analysis.type,
                            complexity: modelSelection.analysis.complexity,
                            estimatedTokens: modelSelection.analysis.estimatedTokens,
                            cacheHit: false,
                            optimizationApplied: []
                        }
                    }];
            });
        });
    };
    /**
     * å¤„ç†åˆ†ææŸ¥è¯¢
     */
    AIOptimizedQueryService.prototype.handleAnalysis = function (queryText, userId, sessionId, modelSelection) {
        var _a, _b, _c;
        return __awaiter(this, void 0, void 0, function () {
            var modelName, analysis, response;
            return __generator(this, function (_d) {
                switch (_d.label) {
                    case 0:
                        modelName = modelSelection.modelName, analysis = modelSelection.analysis;
                        return [4 /*yield*/, ai_bridge_service_1.aiBridgeService.generateChatCompletion({
                                model: modelName,
                                messages: [
                                    {
                                        role: 'system',
                                        content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·æä¾›è¯¦ç»†çš„åˆ†æç»“æœã€‚'
                                    },
                                    {
                                        role: 'user',
                                        content: queryText
                                    }
                                ],
                                temperature: 0.7,
                                max_tokens: analysis.estimatedTokens
                            }, {
                                endpointUrl: modelSelection.modelConfig.endpointUrl,
                                apiKey: modelSelection.modelConfig.apiKey
                            })];
                    case 1:
                        response = _d.sent();
                        return [2 /*return*/, {
                                type: 'ai_response',
                                response: ((_b = (_a = response.choices[0]) === null || _a === void 0 ? void 0 : _a.message) === null || _b === void 0 ? void 0 : _b.content) || 'æ— æ³•åˆ†æ',
                                metadata: {
                                    executionTime: 0,
                                    usedModel: modelName,
                                    queryType: analysis.type,
                                    complexity: analysis.complexity,
                                    estimatedTokens: analysis.estimatedTokens,
                                    actualTokens: ((_c = response.usage) === null || _c === void 0 ? void 0 : _c.total_tokens) || 0,
                                    cacheHit: false,
                                    optimizationApplied: []
                                }
                            }];
                }
            });
        });
    };
    /**
     * å¤„ç†å·¥å…·è°ƒç”¨
     */
    AIOptimizedQueryService.prototype.handleToolCalling = function (queryText, userId, sessionId, modelSelection) {
        return __awaiter(this, void 0, void 0, function () {
            return __generator(this, function (_a) {
                // è¿™é‡Œå¯ä»¥å®ç°å·¥å…·è°ƒç”¨é€»è¾‘
                return [2 /*return*/, {
                        type: 'ai_response',
                        response: "\u5DE5\u5177\u8C03\u7528\u7ED3\u679C: ".concat(queryText),
                        metadata: {
                            executionTime: 0,
                            usedModel: modelSelection.modelName,
                            queryType: modelSelection.analysis.type,
                            complexity: modelSelection.analysis.complexity,
                            estimatedTokens: modelSelection.analysis.estimatedTokens,
                            cacheHit: false,
                            optimizationApplied: []
                        }
                    }];
            });
        });
    };
    /**
     * å¤„ç†é»˜è®¤æŸ¥è¯¢
     */
    AIOptimizedQueryService.prototype.handleDefaultQuery = function (queryText, userId, sessionId, modelSelection) {
        var _a, _b, _c;
        return __awaiter(this, void 0, void 0, function () {
            var response;
            return __generator(this, function (_d) {
                switch (_d.label) {
                    case 0: return [4 /*yield*/, ai_bridge_service_1.aiBridgeService.generateChatCompletion({
                            model: modelSelection.modelName,
                            messages: [
                                {
                                    role: 'system',
                                    content: 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚'
                                },
                                {
                                    role: 'user',
                                    content: queryText
                                }
                            ],
                            temperature: 0.7,
                            max_tokens: modelSelection.analysis.estimatedTokens
                        }, {
                            endpointUrl: modelSelection.modelConfig.endpointUrl,
                            apiKey: modelSelection.modelConfig.apiKey
                        })];
                    case 1:
                        response = _d.sent();
                        return [2 /*return*/, {
                                type: 'ai_response',
                                response: ((_b = (_a = response.choices[0]) === null || _a === void 0 ? void 0 : _a.message) === null || _b === void 0 ? void 0 : _b.content) || 'æ— æ³•å¤„ç†',
                                metadata: {
                                    executionTime: 0,
                                    usedModel: modelSelection.modelName,
                                    queryType: modelSelection.analysis.type,
                                    complexity: modelSelection.analysis.complexity,
                                    estimatedTokens: modelSelection.analysis.estimatedTokens,
                                    actualTokens: ((_c = response.usage) === null || _c === void 0 ? void 0 : _c.total_tokens) || 0,
                                    cacheHit: false,
                                    optimizationApplied: []
                                }
                            }];
                }
            });
        });
    };
    /**
     * æ„å»ºä¼˜åŒ–æç¤ºè¯
     */
    AIOptimizedQueryService.prototype.buildOptimizedPrompt = function (queryText, queryType) {
        switch (queryType) {
            case ai_smart_model_router_service_1.QueryType.COUNT:
                return "\u8BF7\u7EDF\u8BA1\u67E5\u8BE2: ".concat(queryText, "\u3002\u53EA\u9700\u8FD4\u56DE\u6570\u5B57\u6216\u7B80\u8981\u7ED3\u679C\u3002");
            case ai_smart_model_router_service_1.QueryType.STATUS_CHECK:
                return "\u8BF7\u68C0\u67E5\u72B6\u6001: ".concat(queryText, "\u3002\u53EA\u9700\u8FD4\u56DE\"\u662F\"\u6216\"\u5426\"\uFF0C\u6216\u7B80\u77ED\u72B6\u6001\u3002");
            default:
                return queryText;
        }
    };
    /**
     * è·å–æ€§èƒ½ç»Ÿè®¡
     */
    AIOptimizedQueryService.prototype.getPerformanceStats = function () {
        return {
            modelRouter: this.modelRouter.getModelPerformanceStats(),
            cacheStats: this.cacheService.getCacheStats()
        };
    };
    return AIOptimizedQueryService;
}());
exports.AIOptimizedQueryService = AIOptimizedQueryService;
exports["default"] = AIOptimizedQueryService.getInstance();
