#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç£ç›˜ç©ºé—´åˆ†æå™¨
åˆ†ææŒ‡å®šç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µï¼Œæ‰¾å‡ºå ç”¨ç©ºé—´è¾ƒå¤§çš„æ–‡ä»¶å’Œç›®å½•
"""

import os
import sys
import shutil
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
import json
import subprocess


class DiskUsageAnalyzer:
    """ç£ç›˜ç©ºé—´åˆ†æå™¨"""

    def __init__(self, target_path: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            target_path: è¦åˆ†æçš„ç›®æ ‡è·¯å¾„
        """
        self.target_path = Path(target_path) if target_path else Path.home()
        self.total_size = 0
        self.directory_stats = []
        self.large_files = []
        self.file_type_stats = {}
        self.scan_start_time = None
        self.scan_end_time = None

    def format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f}PB"

    def get_directory_size(self, directory: Path) -> int:
        """è·å–ç›®å½•å¤§å°ï¼ˆä½¿ç”¨duå‘½ä»¤ä»¥æé«˜æ€§èƒ½ï¼‰"""
        try:
            result = subprocess.run(
                ['du', '-sb', str(directory)],
                capture_output=True,
                text=True,
                check=True
            )
            return int(result.stdout.split()[0])
        except (subprocess.CalledProcessError, FileNotFoundError):
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨os.walk
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(directory):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    try:
                        total_size += os.path.getsize(filepath)
                    except (OSError, PermissionError):
                        continue
            return total_size

    def should_ignore_path(self, path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¿½ç•¥è¯¥è·¯å¾„"""
        ignore_patterns = {
            '.git', 'node_modules', '.npm', '.cache', '__pycache__',
            'coverage', '.pytest_cache', '.nyc_output', '.vscode',
            '.idea', 'vendor', '.venv', 'venv', 'env', 'dist', 'build'
        }

        return any(pattern in path.name for pattern in ignore_patterns)

    def analyze_directory_structure(self, max_depth: int = 3) -> None:
        """åˆ†æç›®å½•ç»“æ„"""
        print(f"ğŸ” åˆ†æç›®å½•ç»“æ„: {self.target_path}")

        def scan_directory(directory: Path, current_depth: int = 0):
            if current_depth >= max_depth:
                return

            try:
                items = list(directory.iterdir())
            except (OSError, PermissionError):
                return

            for item in items:
                if self.should_ignore_path(item):
                    continue

                if item.is_dir():
                    try:
                        size = self.get_directory_size(item)
                        self.directory_stats.append({
                            'path': str(item.relative_to(self.target_path)),
                            'absolute_path': str(item),
                            'size_bytes': size,
                            'size_formatted': self.format_size(size),
                            'depth': current_depth + 1,
                            'type': 'directory'
                        })
                        self.total_size += size

                        # é€’å½’æ‰«æå­ç›®å½•
                        scan_directory(item, current_depth + 1)
                    except (OSError, PermissionError):
                        continue

        scan_directory(self.target_path)

    def find_large_files(self, min_size_mb: int = 100) -> None:
        """æŸ¥æ‰¾å¤§æ–‡ä»¶"""
        print(f"ğŸ” æŸ¥æ‰¾å¤§äº {min_size_mb}MB çš„æ–‡ä»¶...")

        min_size_bytes = min_size_mb * 1024 * 1024

        for root, dirs, files in os.walk(self.target_path):
            # è·³è¿‡å¿½ç•¥çš„ç›®å½•
            dirs[:] = [d for d in dirs if not self.should_ignore_path(Path(d))]

            for filename in files:
                filepath = Path(root) / filename
                try:
                    if filepath.is_file() and not filepath.is_symlink():
                        size = filepath.stat().st_size
                        if size >= min_size_bytes:
                            self.large_files.append({
                                'path': str(filepath.relative_to(self.target_path)),
                                'absolute_path': str(filepath),
                                'size_bytes': size,
                                'size_mb': size / (1024 * 1024),
                                'size_formatted': self.format_size(size),
                                'modified_time': datetime.fromtimestamp(filepath.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                                'file_type': filepath.suffix.lower() or 'æ— æ‰©å±•å'
                            })

                            # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                            file_type = filepath.suffix.lower() or 'æ— æ‰©å±•å'
                            if file_type not in self.file_type_stats:
                                self.file_type_stats[file_type] = {'count': 0, 'size': 0}
                            self.file_type_stats[file_type]['count'] += 1
                            self.file_type_stats[file_type]['size'] += size

                except (OSError, PermissionError):
                    continue

    def analyze_file_types(self) -> None:
        """åˆ†ææ–‡ä»¶ç±»å‹åˆ†å¸ƒ"""
        print("ğŸ” åˆ†ææ–‡ä»¶ç±»å‹åˆ†å¸ƒ...")

        type_counts = {}
        type_sizes = {}

        for root, dirs, files in os.walk(self.target_path):
            # è·³è¿‡å¿½ç•¥çš„ç›®å½•
            dirs[:] = [d for d in dirs if not self.should_ignore_path(Path(d))]

            for filename in files:
                filepath = Path(root) / filename
                try:
                    if filepath.is_file() and not filepath.is_symlink():
                        size = filepath.stat().st_size
                        file_type = filepath.suffix.lower() or 'æ— æ‰©å±•å'

                        if file_type not in type_counts:
                            type_counts[file_type] = 0
                            type_sizes[file_type] = 0

                        type_counts[file_type] += 1
                        type_sizes[file_type] += size
                except (OSError, PermissionError):
                    continue

        self.file_type_stats = {
            file_type: {
                'count': type_counts[file_type],
                'size_bytes': type_sizes[file_type],
                'size_formatted': self.format_size(type_sizes[file_type])
            }
            for file_type in type_counts
        }

    def run_full_analysis(self, min_file_size_mb: int = 100, max_depth: int = 3) -> None:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        self.scan_start_time = datetime.now()

        print("ğŸš€ å¼€å§‹ç£ç›˜ç©ºé—´åˆ†æ...")
        print(f"ğŸ“ ç›®æ ‡è·¯å¾„: {self.target_path}")
        print(f"ğŸ“ æœ€å°æ–‡ä»¶å¤§å°: {min_file_size_mb}MB")
        print(f"ğŸ“ æœ€å¤§æ‰«ææ·±åº¦: {max_depth}")
        print("=" * 60)

        # åˆ†æç›®å½•ç»“æ„
        self.analyze_directory_structure(max_depth)

        # æŸ¥æ‰¾å¤§æ–‡ä»¶
        self.find_large_files(min_file_size_mb)

        # åˆ†ææ–‡ä»¶ç±»å‹
        self.analyze_file_types()

        self.scan_end_time = datetime.now()

        print(f"\nâœ… åˆ†æå®Œæˆï¼è€—æ—¶: {self.scan_end_time - self.scan_start_time}")

    def generate_report(self, output_format: str = 'text') -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if output_format == 'json':
            return self._generate_json_report()
        else:
            return self._generate_text_report()

    def _generate_text_report(self) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        report_lines = []

        # æŠ¥å‘Šæ ‡é¢˜
        report_lines.append("=" * 80)
        report_lines.append("ğŸ’¾ ç£ç›˜ç©ºé—´åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ğŸ“ åˆ†æè·¯å¾„: {self.target_path}")
        report_lines.append(f"ğŸ“… åˆ†ææ—¶é—´: {self.scan_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"â±ï¸  åˆ†æè€—æ—¶: {self.scan_end_time - self.scan_start_time}")
        report_lines.append("")

        # æ€»ä½“ç»Ÿè®¡
        report_lines.append("ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        report_lines.append(f"  â€¢ æ€»å¤§å°: {self.format_size(self.total_size)}")
        report_lines.append(f"  â€¢ ç›®å½•æ•°é‡: {len(self.directory_stats)}")
        report_lines.append(f"  â€¢ å¤§æ–‡ä»¶æ•°é‡: {len(self.large_files)}")
        report_lines.append(f"  â€¢ æ–‡ä»¶ç±»å‹æ•°é‡: {len(self.file_type_stats)}")
        report_lines.append("")

        # å¤§æ–‡ä»¶åˆ—è¡¨
        if self.large_files:
            report_lines.append("ğŸš¨ å¤§æ–‡ä»¶åˆ—è¡¨:")
            report_lines.append("-" * 80)
            sorted_files = sorted(self.large_files, key=lambda x: x['size_bytes'], reverse=True)
            report_lines.append(f"{'åºå·':<4} {'å¤§å°':<12} {'ç±»å‹':<8} {'ä¿®æ”¹æ—¶é—´':<20} {'è·¯å¾„'}")
            report_lines.append("-" * 80)

            for i, file_info in enumerate(sorted_files[:20], 1):  # åªæ˜¾ç¤ºå‰20ä¸ª
                report_lines.append(
                    f"{i:<4} {file_info['size_formatted']:<12} {file_info['file_type']:<8} "
                    f"{file_info['modified_time']:<20} {file_info['path']}"
                )
            report_lines.append("")

        # ç›®å½•å¤§å°æ’å
        if self.directory_stats:
            report_lines.append("ğŸ“ ç›®å½•å¤§å°æ’å:")
            report_lines.append("-" * 60)
            sorted_dirs = sorted(self.directory_stats, key=lambda x: x['size_bytes'], reverse=True)
            report_lines.append(f"{'æ’å':<4} {'å¤§å°':<12} {'è·¯å¾„'}")
            report_lines.append("-" * 60)

            for i, dir_info in enumerate(sorted_dirs[:15], 1):  # åªæ˜¾ç¤ºå‰15ä¸ª
                report_lines.append(
                    f"{i:<4} {dir_info['size_formatted']:<12} {dir_info['path']}"
                )
            report_lines.append("")

        # æ–‡ä»¶ç±»å‹åˆ†æ
        if self.file_type_stats:
            report_lines.append("ğŸ“ˆ æ–‡ä»¶ç±»å‹åˆ†æ:")
            report_lines.append("-" * 60)
            sorted_types = sorted(self.file_type_stats.items(),
                                key=lambda x: x[1]['size_bytes'], reverse=True)
            report_lines.append(f"{'ç±»å‹':<12} {'æ–‡ä»¶æ•°é‡':<10} {'æ€»å¤§å°':<15} {'å¹³å‡å¤§å°':<12}")
            report_lines.append("-" * 60)

            for file_type, stats in sorted_types[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                avg_size = stats['size_bytes'] / stats['count'] if stats['count'] > 0 else 0
                avg_size_str = self.format_size(avg_size)
                report_lines.append(
                    f"{file_type or 'æ— æ‰©å±•å':<12} {stats['count']:<10} "
                    f"{stats['size_formatted']:<15} {avg_size_str:<12}"
                )
            report_lines.append("")

        # ç£ç›˜ç©ºé—´å»ºè®®
        disk_usage = shutil.disk_usage(str(self.target_path))
        free_space_gb = disk_usage.free / (1024**3)
        usage_percent = (disk_usage.used / disk_usage.total) * 100

        report_lines.append("ğŸ’¾ ç£ç›˜ç©ºé—´çŠ¶å†µ:")
        report_lines.append(f"  â€¢ æ€»å®¹é‡: {self.format_size(disk_usage.total)}")
        report_lines.append(f"  â€¢ å·²ä½¿ç”¨: {self.format_size(disk_usage.used)} ({usage_percent:.1f}%)")
        report_lines.append(f"  â€¢ å¯ç”¨ç©ºé—´: {self.format_size(disk_usage.free)}")
        report_lines.append("")

        report_lines.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if free_space_gb < 10:
            report_lines.append("  âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³ï¼å»ºè®®ç«‹å³æ¸…ç†ï¼š")
        elif free_space_gb < 20:
            report_lines.append("  âš ï¸  ç£ç›˜ç©ºé—´è¾ƒå°‘ï¼Œå»ºè®®æ¸…ç†ï¼š")
        else:
            report_lines.append("  âœ… ç£ç›˜ç©ºé—´å……è¶³ï¼Œä½†å¯ä»¥è€ƒè™‘ä¼˜åŒ–ï¼š")

        report_lines.append("  â€¢ æ¸…ç†æ—¥å¿—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶")
        report_lines.append("  â€¢ åˆ é™¤ä¸éœ€è¦çš„å¤‡ä»½æ–‡ä»¶")
        report_lines.append("  â€¢ å‹ç¼©æˆ–å½’æ¡£ä¸å¸¸ç”¨çš„æ–‡ä»¶")
        report_lines.append("  â€¢ æ£€æŸ¥é‡å¤æ–‡ä»¶")

        return "\n".join(report_lines)

    def _generate_json_report(self) -> str:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report_data = {
            'analysis_info': {
                'target_path': str(self.target_path),
                'scan_start_time': self.scan_start_time.isoformat(),
                'scan_end_time': self.scan_end_time.isoformat(),
                'total_size_bytes': self.total_size,
                'total_size_formatted': self.format_size(self.total_size)
            },
            'directory_stats': sorted(self.directory_stats,
                                    key=lambda x: x['size_bytes'], reverse=True),
            'large_files': sorted(self.large_files,
                                key=lambda x: x['size_bytes'], reverse=True),
            'file_type_stats': self.file_type_stats
        }
        return json.dumps(report_data, ensure_ascii=False, indent=2)

    def save_report(self, report: str, filename: str = None) -> str:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"disk_usage_report_{timestamp}.txt"

        report_path = Path(filename)
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            return str(report_path.absolute())
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥ï¼š{e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ç£ç›˜ç©ºé—´åˆ†æå™¨')
    parser.add_argument('path', nargs='?',
                       default='/persistent/home/zhgue',
                       help='è¦åˆ†æçš„è·¯å¾„ (é»˜è®¤: /persistent/home/zhgue)')
    parser.add_argument('-s', '--min-size', type=int, default=100,
                       help='æœ€å°æ–‡ä»¶å¤§å°(MB) (é»˜è®¤: 100)')
    parser.add_argument('-d', '--max-depth', type=int, default=3,
                       help='æœ€å¤§ç›®å½•æ·±åº¦ (é»˜è®¤: 3)')
    parser.add_argument('-f', '--format', choices=['text', 'json'],
                       default='text', help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: text)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = DiskUsageAnalyzer(args.path)

    # è¿è¡Œåˆ†æ
    analyzer.run_full_analysis(args.min_size, args.max_depth)

    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report(args.format)

    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + report)

    # ä¿å­˜æŠ¥å‘Š
    if not args.no_save:
        saved_path = analyzer.save_report(report, args.output)
        if saved_path:
            print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {saved_path}")


if __name__ == "__main__":
    main()